{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TEXT_ANALYSIS PREPROCESSING_STEPS.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "busaqPkRq96V"
      },
      "source": [
        "#1.Lower casing\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbxjT-ncqfvP"
      },
      "source": [
        "df[\"text_lower\"] = df[\"text\"].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heVBxcd4rHTR"
      },
      "source": [
        "# 2.removal of punctuations\r\n",
        "\r\n",
        "Punctuation removal might be a good step, when punctuation does not brings additional value for text vectorization. Punctuation removal is better to be done after the tokenization step, doing it before might cause undesirable effects. Good choice for TF-IDF, Count, Binary vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFiEyXRDrF46"
      },
      "source": [
        "def remove_punctuation(text):\r\n",
        "  no_punct=\"\".join([c for c i text if c not in string.punctuation])\r\n",
        "  return no_punct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J--XtKIirFt5"
      },
      "source": [
        "df['text']=df['text'].apply(lambda x: remove_punctuation(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1J9mFva6sXm"
      },
      "source": [
        "or"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYnwS3DfrFjD"
      },
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\r\n",
        "def remove_punctuation(text):\r\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\r\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\r\n",
        "\r\n",
        "df[\"text_wo_punct\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlPEVBDb7pyq"
      },
      "source": [
        "#2.Removal of stopwordsÂ¶\r\n",
        "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\r\n",
        "\r\n",
        "These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEb39Y1PrFXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "1cab28a5-41b9-47b2-fdf4-a94d4cfc78fc"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\", \".join(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXYi3YMfrE84"
      },
      "source": [
        "#removing dem\r\n",
        "STOPWORDS = set(stopwords.words('english'))\r\n",
        "def remove_stopwords(text):\r\n",
        "    \"\"\"custom function to remove the stopwords\"\"\"\r\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\r\n",
        "\r\n",
        "df[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jXZlt51O_rI"
      },
      "source": [
        "###second example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk6_aibXO_Aw"
      },
      "source": [
        "def remove_stopwords(text):\r\n",
        "  words=[w for w in text if w not in stopwords.words('english')]\r\n",
        "  return words\r\n",
        "\r\n",
        "  df['text']=df['text'].apply(lambda x:remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVUO1sTz8xsL"
      },
      "source": [
        "#3.Removal of Frequent words\r\n",
        "In the previos preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us.\r\n",
        "\r\n",
        "So this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of.\r\n",
        "\r\n",
        "Let us get the most common words adn then remove them in the next step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JmoDMWA8bMZ"
      },
      "source": [
        "from collections import Counter\r\n",
        "cnt = Counter()\r\n",
        "for text in df[\"text_wo_stop\"].values:\r\n",
        "    for word in text.split():\r\n",
        "        cnt[word] += 1\r\n",
        "        \r\n",
        "cnt.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byEvS5L18bKv"
      },
      "source": [
        "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\r\n",
        "def remove_freqwords(text):\r\n",
        "    \"\"\"custom function to remove the frequent words\"\"\"\r\n",
        "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\r\n",
        "\r\n",
        "df[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV-51dzM9wB1"
      },
      "source": [
        "#4.Removal of Rare wordsÂ¶\r\n",
        "This is very similar to previous preprocessing step but we will remove the rare words from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsqhFAqW8bHm"
      },
      "source": [
        "# Drop the two columns which are no more needed \r\n",
        "df.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\r\n",
        "\r\n",
        "n_rare_words = 10\r\n",
        "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\r\n",
        "def remove_rarewords(text):\r\n",
        "    \"\"\"custom function to remove the rare words\"\"\"\r\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\r\n",
        "\r\n",
        "df[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmf8gfHN-ca9"
      },
      "source": [
        "#5.StemmingÂ¶\r\n",
        "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From Wikipedia)\r\n",
        "\r\n",
        "For example, if there are two words in the corpus walks and walking, then stemming will stem the suffix to make them walk. But say in another example, we have two words console and consoling, the stemmer will remove the suffix and make them consol which is not a proper english word.\r\n",
        "\r\n",
        "There are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7BWRbVn8bE0"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\r\n",
        "\r\n",
        "# Drop the two columns \r\n",
        "df.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \r\n",
        "\r\n",
        "stemmer = PorterStemmer()\r\n",
        "def stem_words(text):\r\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\r\n",
        "\r\n",
        "df[\"text_stemmed\"] = df[\"text\"].apply(lambda text: stem_words(text))\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfcCM1UR_HBH"
      },
      "source": [
        "We can see that words like private and propose have their e at the end chopped off due to stemming. This is not intented. What can we do fort hat? We can use Lemmatization in such cases.\r\n",
        "\r\n",
        "Also this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eChkFfeT8bBb",
        "outputId": "0be300b8-26c8-403d-b48c-7a40aca8f0fa"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "SnowballStemmer.languages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdLXQj1g_qMD"
      },
      "source": [
        "#6.Lemmatization\r\n",
        "Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language.\r\n",
        "\r\n",
        "As a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization.\r\n",
        "\r\n",
        "Let us use the WordNetLemmatizer in nltk to lemmatize our sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1XnzGBd8a_z"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "def lemmatize_words(text):\r\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\r\n",
        "\r\n",
        "df[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFe1HmDnREeS"
      },
      "source": [
        "###second example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb5ZzrH3RJ88"
      },
      "source": [
        "#instantiate initializer\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "def lemmatize_words(text):\r\n",
        "  lem=[lemmatizer.lemmatize(i) for i in text]\r\n",
        "  return lem\r\n",
        "\r\n",
        "df['text']=df['text'].apply(lambda x:lemmatize_words(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k2ShXnoBLk_"
      },
      "source": [
        "We can see that the trailing e in the propose and private is retained when we use lemmatization unlike stemming.\r\n",
        "\r\n",
        "Wait. There is one more thing in lemmatization. Let us try to lemmatize running now.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkm_AYZi8a75"
      },
      "source": [
        "\r\n",
        "lemmatizer.lemmatize(\"running\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjH_3B5kBwjy"
      },
      "source": [
        "Wow. It returned running as such without converting it to the root form run. This is because the lemmatization process depends on the POS tag to come up with the correct lemma. Now let us lemmatize again by providing the POS tag for the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrSna02e8a6W"
      },
      "source": [
        "lemmatizer.lemmatize(\"running\", \"v\") # v for verb\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ivUKeHfB6qE"
      },
      "source": [
        "Now we are getting the root form run. So we also need to provide the POS tag of the word along with the word for lemmatizer in nltk. Depending on the POS, the lemmatizer may return different results.\r\n",
        "\r\n",
        "Let us take the example, stripes and check the lemma when it is both verb and noun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MendzRV18a13"
      },
      "source": [
        "print(\"Word is : stripes\")\r\n",
        "print(\"Lemma result for verb : \",lemmatizer.lemmatize(\"stripes\", 'v'))\r\n",
        "print(\"Lemma result for noun : \",lemmatizer.lemmatize(\"stripes\", 'n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjd4PQIZClGe"
      },
      "source": [
        "Word is : stripes\r\n",
        "\r\n",
        "Lemma result for verb :  strip\r\n",
        "\r\n",
        "Lemma result for noun :  stripe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llfo4szvC4EN"
      },
      "source": [
        "#Now let us redo the lemmatization process for our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP-fMrb78avz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "from nltk.corpus import wordnet\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\r\n",
        "def lemmatize_words(text):\r\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\r\n",
        "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\r\n",
        "\r\n",
        "df[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM3irh5YDKnI"
      },
      "source": [
        "We can now see that in the third row, sent got converted to send since we provided the POS tag for lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq7RXzAYDyxt"
      },
      "source": [
        "#7.Removal of EmojisÂ¶\r\n",
        "With more and more usage of social media platforms, there is an explosion in the usage of emojis in our day to day life as well. Probably we might need to remove these emojis for some of our textual analysis.\r\n",
        "\r\n",
        "Thanks to https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b, please find below a helper function to remove emojis from our text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt05qCrU8aoQ"
      },
      "source": [
        "def remove_emoji(string):\r\n",
        "    emoji_pattern = re.compile(\"[\"\r\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           u\"\\U00002702-\\U000027B0\"\r\n",
        "                           u\"\\U000024C2-\\U0001F251\"\r\n",
        "                           \"]+\", flags=re.UNICODE)\r\n",
        "    return emoji_pattern.sub(r'', string)\r\n",
        "\r\n",
        "remove_emoji(\"game is on ðŸ”¥ðŸ”¥\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3roAMH5FC_3J"
      },
      "source": [
        "remove_emoji(\"HilariousðŸ˜‚\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0tG7JbbDBuV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQCRn0YPEufw"
      },
      "source": [
        "#8.Removal of Emoticons\r\n",
        "This is what we did in the last step right? Nope. We did remove emojis in the last step but not emoticons. There is a minor difference between emojis and emoticons.\r\n",
        "\r\n",
        "From Grammarist.com, emoticon is built from keyboard characters that when put together in a certain way represent a facial expression, an emoji is an actual image.\r\n",
        "\r\n",
        ":-) is an emoticon\r\n",
        "\r\n",
        "ðŸ˜€ is an emoji\r\n",
        "\r\n",
        "Thanks to NeelShah for the wonderful collection of emoticons, we are going to use them to remove emoticons.\r\n",
        "\r\n",
        "Please note again that the removal of emojis / emoticons are not always preferred and decision should be made based on the use case at hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "969SFx2JDBko"
      },
      "source": [
        "def remove_emoticons(text):\r\n",
        "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\r\n",
        "    return emoticon_pattern.sub(r'', text)\r\n",
        "\r\n",
        "remove_emoticons(\"Hello :-)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMyfvynKDBYW"
      },
      "source": [
        "#ANOTHER EXAMPLE\r\n",
        "remove_emoticons(\"I am sad :(\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnIRVYUtGRv8"
      },
      "source": [
        "#9.Conversion of Emoticon to Words\r\n",
        "In the previous step, we have removed the emoticons. In case of use cases like sentiment analysis, the emoticons give some valuable information and so removing them might not be a good solution. What can we do in such cases?\r\n",
        "\r\n",
        "One way is to convert the emoticons to word format so that they can be used in downstream modeling processes. Thanks for Neel again for the wonderful dictionary that we have used in the previous step. We are going to use that again for conversion of emoticons to words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydslaSamDBBr"
      },
      "source": [
        "def convert_emoticons(text):\r\n",
        "    for emot in EMOTICONS:\r\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\r\n",
        "    return text\r\n",
        "\r\n",
        "text = \"Hello :-) :-)\"\r\n",
        "convert_emoticons(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-XhJZOjDAYW"
      },
      "source": [
        "text = \"I am sad :()\"\r\n",
        "convert_emoticons(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih8RU1iMHBBI"
      },
      "source": [
        "###This method might be better for some use cases when we do not want to miss out on the emoticon information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihJqPUkqHPDZ"
      },
      "source": [
        "#10.Conversion of Emoji to WordsÂ¶\r\n",
        "Now let us do the same for Emojis as well. Neel Shah has put together a list of emojis with the corresponding words as well as part of his https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py. We are going to make use of this dictionary to convert the emojis to corresponding words.\r\n",
        "\r\n",
        "Again this conversion might be better than emoji removal for certain use cases. Please use the one that is suitable for the use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhXRCmZFG4vc"
      },
      "source": [
        "def convert_emojis(text):\r\n",
        "    for emot in UNICODE_EMO:\r\n",
        "        text = re.sub(r'('+emot+')', \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\r\n",
        "    return text\r\n",
        "\r\n",
        "text = \"game is on ðŸ”¥\"\r\n",
        "convert_emojis(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8a86JdxIrlo"
      },
      "source": [
        "#11.Removal of URLs\r\n",
        "Next preprocessing step is to remove any URLs present in the data. For example, if we are doing a twitter analysis, then there is a good chance that the tweet will have some URL in it. Probably we might need to remove them for our further analysis.\r\n",
        "\r\n",
        "We can use the below code snippet to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_1hmHWsG5ZR"
      },
      "source": [
        "def remove_urls(text):\r\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\r\n",
        "    return url_pattern.sub(r'', text)\r\n",
        "text = \"Driverless AI NLP blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\r\n",
        "remove_urls(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "667_5ZocJUeI"
      },
      "source": [
        "###Thanks to Pranjal for the edge cases in the comments below. Suppose say there is no http or https in the url link. The function can now captures that as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk36K7rtG5ul"
      },
      "source": [
        "text = \"Want to know more. Checkout www.h2o.ai for additional information\"\r\n",
        "remove_urls(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG50bJRaJu7j"
      },
      "source": [
        "#12.Removal of HTML TagsÂ¶\r\n",
        "One another common preprocessing technique that will come handy in multiple places is removal of html tags. This is especially useful, if we scrap the data from different websites. We might end up having html strings as part of our text.\r\n",
        "\r\n",
        "First, let us try to remove the HTML tags using regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10c1aClNG6DF"
      },
      "source": [
        "def remove_html(text):\r\n",
        "    html_pattern = re.compile('<.*?>')\r\n",
        "    return html_pattern.sub(r'', text)\r\n",
        "\r\n",
        "text = \"\"\"<div>\r\n",
        "<h1> H2O</h1>\r\n",
        "<p> AutoML</p>\r\n",
        "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\r\n",
        "</div>\"\"\"\r\n",
        "\r\n",
        "print(remove_html(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNkJuzohKPAS"
      },
      "source": [
        "### OUTPUT>>\r\n",
        " H2O\r\n",
        "\r\n",
        " AutoML\r\n",
        "\r\n",
        " Driverless AI\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqVHH6tzKozF"
      },
      "source": [
        "We can also use BeautifulSoup package to get the text from HTML document in a more elegant way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFPjqIb8G6U-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634fee96-54bd-4f6e-d0b2-fbd6705cc6d4"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def remove_html(text):\r\n",
        "    return BeautifulSoup(text, \"lxml\").text\r\n",
        "\r\n",
        "text = \"\"\"<div>\r\n",
        "<h1> H2O</h1>\r\n",
        "<p> AutoML</p>\r\n",
        "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\r\n",
        "</div>\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "print(remove_html(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " H2O\n",
            " AutoML\n",
            " Driverless AI\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7v8h2C5CA9l"
      },
      "source": [
        "##Removal of numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkjFFIcYCHqQ"
      },
      "source": [
        "def remove_numbers(text):\r\n",
        "    # define the pattern to keep\r\n",
        "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \r\n",
        "    return re.sub(pattern, '', text)\r\n",
        "\r\n",
        "tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda a:remove_numbers(a) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0bTBUJ9K6LA"
      },
      "source": [
        "#13.Chat Words Conversion\r\n",
        "This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes.\r\n",
        "\r\n",
        "Got a good list of chat slang words from this repo::https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt. We can use this for our conversion here. We can add more words to this list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_BBnq5cKvwf"
      },
      "source": [
        "chat_words_map_dict = {}\r\n",
        "chat_words_list = []\r\n",
        "for line in chat_words_str.split(\"\\n\"):\r\n",
        "    if line != \"\":\r\n",
        "        cw = line.split(\"=\")[0]\r\n",
        "        cw_expanded = line.split(\"=\")[1]\r\n",
        "        chat_words_list.append(cw)\r\n",
        "        chat_words_map_dict[cw] = cw_expanded\r\n",
        "chat_words_list = set(chat_words_list)\r\n",
        "\r\n",
        "def chat_words_conversion(text):\r\n",
        "    new_text = []\r\n",
        "    for w in text.split():\r\n",
        "        if w.upper() in chat_words_list:\r\n",
        "            new_text.append(chat_words_map_dict[w.upper()])\r\n",
        "        else:\r\n",
        "            new_text.append(w)\r\n",
        "    return \" \".join(new_text)\r\n",
        "\r\n",
        "chat_words_conversion(\"one minute BRB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKFnuCN-KwZ_"
      },
      "source": [
        "chat_words_conversion(\"imo this is awesome\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNpsethhMAmQ"
      },
      "source": [
        "We can add more words to our abbreviation list and use them based on our use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AYkHuh7MUWG"
      },
      "source": [
        "#14.Spelling CorrectionÂ¶\r\n",
        "One another important text preprocessing step is spelling correction. Typos are common in text data and we might want to correct those spelling mistakes before we do our analysis.\r\n",
        "\r\n",
        "If we are interested in wrinting a spell corrector of our own, we can probably start with famous code:https://norvig.com/spell-correct.html from Peter Norvig.\r\n",
        "\r\n",
        "In this notebook, let us use the python package pyspellchecker for our spelling correction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pge5gqdQKwr_"
      },
      "source": [
        "from spellchecker import SpellChecker\r\n",
        "\r\n",
        "spell = SpellChecker()\r\n",
        "def correct_spellings(text):\r\n",
        "    corrected_text = []\r\n",
        "    misspelled_words = spell.unknown(text.split())\r\n",
        "    for word in text.split():\r\n",
        "        if word in misspelled_words:\r\n",
        "            corrected_text.append(spell.correction(word))\r\n",
        "        else:\r\n",
        "            corrected_text.append(word)\r\n",
        "    return \" \".join(corrected_text)\r\n",
        "        \r\n",
        "text = \"speling correctin\"\r\n",
        "correct_spellings(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBjQ2shb1hG0"
      },
      "source": [
        "https://www.geeksforgeeks.org/python-textblob-correct-method/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Veu87PqW1i5T"
      },
      "source": [
        "#Let's practice spelling and word harmonization\r\n",
        "from textblob import TextBlob\r\n",
        "tweet_df['text']=tweet_df['text'].apply(lambda x: str(TextBlob(x).correct()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ulDxEMW_eF"
      },
      "source": [
        "#15.Spacy text without stop words:\r\n",
        "Here you see that nltk and spacy have different vocabulary size, so the results of filtering are different. But the main thing I want to underline is that the word not was filtered, which in most cases will be alright, but in the case when you want to determine the polarity of this sentence not will bring additional meaning.\r\n",
        "For such cases, you are able to set stop words you can ignore in spacy library. In the case of nltk you cat just remove or add custom words to nltk_stop_words, it is just a list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOJ3Ki_-Xqkf"
      },
      "source": [
        "source:https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHlV7wC8Kw24"
      },
      "source": [
        "import spacy\r\n",
        "import en_core_web_sm\r\n",
        "\r\n",
        "nlp = en_core_web_sm.load()\r\n",
        "text_without_stop_words = [t.text for t in nlp(text) if not t.is_stop]\r\n",
        "display(f\"Spacy text without stop words: {text_without_stop_words}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiYOf5QUKw-_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNyWWO0PA2fe"
      },
      "source": [
        "#16.Removing non_ascii characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN1d_BCLBBSo"
      },
      "source": [
        "!pip install unicodedata2\r\n",
        "import unicodedata2\r\n",
        "def remove_non_ascii(words):\r\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\r\n",
        "    new_words = []\r\n",
        "    for word in words:\r\n",
        "        new_word = unicodedata2.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\r\n",
        "        new_words.append(new_word)\r\n",
        "    return new_words\r\n",
        "tweet_df[\"text\"] = tweet_df[\"text\"].apply(lambda w:remove_non_ascii(w) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CscF_gYNDXd-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRMGWEgSDYQc"
      },
      "source": [
        "SOURCE:https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P1D42hzDW7J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG9_uzEjI_FM"
      },
      "source": [
        "#17.Word Embedding Techniques\r\n",
        "There is a huge amount of data in text format. Analyzing text data is an extremely complex task for a machine as itâ€™s difficult for a machine to understand the semantics behind text. At this stage, weâ€™re going to process our text data into a machine-understandable format using word embedding.\r\n",
        "\r\n",
        "    Word Embedding is simply converting data in a text format to numerical values (or vectors) so we can give these vectors  as input to a machine, and analyze the data using the concepts of algebra.\r\n",
        "However, itâ€™s important to note that when we perform this transformation there could be data loss. The key then is to maintain an equilibrium between conversion and retaining data.\r\n",
        "\r\n",
        "Here are two commonly used terminologies when it comes to this step.\r\n",
        "\r\n",
        "    Each text data point is called a Document\r\n",
        "    An entire set of documents is called a Corpus\r\n",
        "Text processing can be done using the following techniques:\r\n",
        "\r\n",
        "\r\n",
        "    Bag of Words\r\n",
        "\r\n",
        "    TF-IDF\r\n",
        "\r\n",
        "    Word2Vec\r\n",
        "\r\n",
        "Next, letâ€™s explore each of the above techniques in more detail, then decide which to use for our Twitter sentiment analysis model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReDqSsgXLZWa"
      },
      "source": [
        "##17 (A). Bag of WordsÂ¶\r\n",
        "Bag of Words does a simple transformation of the document to a vector by using a dictionary of unique words. This is done in just two steps, outlined below.\r\n",
        "\r\n",
        "Construction of Dictionary\r\n",
        "\r\n",
        "Create a dictionary of all the unique words in the data corpus in a vector form. Let the number of unique words in the corpus be, â€˜dâ€™. So each word is a dimension and hence this dictionary vector is a d-dimension vector.\r\n",
        "\r\n",
        "Construction of Vectors\r\n",
        "\r\n",
        "For each document, say, ráµ¢ we create a vector, say, váµ¢.\r\n",
        "\r\n",
        "     This váµ¢ which has d-dimensions can be constructed in two ways:\r\n",
        "1.For each document, the váµ¢ is constructed in accordance with the dictionary such that each word in the dictionary is reproduced by the number of times that word is present in the document.\r\n",
        "\r\n",
        "2.For each document, the váµ¢ is constructed in accordance with the dictionary such that each word in the dictionary is reproduced as:\r\n",
        "\r\n",
        "    1 if the word exists in the document or\r\n",
        "    0 if the word doesnâ€™t exist in the document\r\n",
        "This type is known as a Binary Bag of Words.\r\n",
        "\r\n",
        "Now we have vectors for each document and a dictionary with a set of unique words from the data corpus. These vectors can be analyzed by,\r\n",
        "\r\n",
        "    plotting in d-dimension space or\r\n",
        "    calculating distance between vectors to get the similarity (the closer the vectors are, the more similar they are)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM49ygi1lVUv"
      },
      "source": [
        "#1.WORD2VEC MODEL\r\n",
        "In Word2Vec every word is assigned a vector. We start with either a random vector or one-hot vector.\r\n",
        "\r\n",
        "One-Hot vector: A representation where only one bit in a vector is 1.If there are 500 words in the corpus then the vector length will be 500. After assigning vectors to each word we take a window size and iterate through the entire corpus. While we do this there are two neural embedding methods which are used:\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "Word2Vec consists of models for generating word embedding. These models are shallow two layer neural networks having one input layer, one hidden layer and one output layer. Word2Vec utilizes two architectures :\r\n",
        "\r\n",
        "\r\n",
        "###-CBOW\r\n",
        "\r\n",
        " CBOW (Continuous Bag of Words) : CBOW model predicts the current word given context words within specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent current word present at the output layer.\r\n",
        "\r\n",
        "###-SKIPGRAM\r\n",
        " Skip Gram : Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer.\r\n",
        "\r\n",
        "\r\n",
        "#Goal of Word Embeddings\r\n",
        "\r\n",
        "To reduce dimensionality\r\n",
        "\r\n",
        "To use a word to predict the words around it\r\n",
        "\r\n",
        "Inter word semantics must be captured"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnR3G0nxKxjS"
      },
      "source": [
        "import gensim \r\n",
        "from gensim.models import Word2Vec \r\n",
        "# Create CBOW model \r\n",
        "#by default sg=cbow\r\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5) \r\n",
        "\r\n",
        "\r\n",
        "# Create Skip Gram model \r\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXlr_-dWrtAO"
      },
      "source": [
        "#2) GloVe:\r\n",
        "This is another method for creating word embeddings. In this method, we take the corpus and iterate through it and get the co-occurence of each word with other words in the corpus. We get a co-occurence matrix through this. The words which occur next to each other get a value of 1, if they are one word apart then 1/2, if two words apart then 1/3 and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZxzK9v27yej"
      },
      "source": [
        "#18.Extracting Features from Cleaned TweetsÂ¶\r\n",
        "To analyze a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques â€“ Bag-of-Words, TF-IDF, and Word Embeddings. In this article, we will be covering only Bag-of-Words and TF-IDF.\r\n",
        "\r\n",
        "#Bag-of-Words FeaturesÂ¶\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5wL_z_P7yGn"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\r\n",
        "# bag-of-words feature matrix\r\n",
        "bow = bow_vectorizer.fit_transform(dataset['Tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEF3LppX8OZ2"
      },
      "source": [
        "#TF-IDF FeaturesÂ¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGbxrLS7rXv7"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\r\n",
        "# TF-IDF feature matrix\r\n",
        "tfidf = tfidf_vectorizer.fit_transform(dataset['Tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYurKPfUrXmR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdcQTXqq9Igg"
      },
      "source": [
        "#more on TF-IDF AND BAG OF WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o70Bh1W_rXjI"
      },
      "source": [
        "#Vectorization\r\n",
        "#Importing Required Packages\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\r\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1_z8l_IrXfE"
      },
      "source": [
        "#Applying Bag of Words Vectorization to the Tweets\r\n",
        "bow_vectorizer = CountVectorizer(stop_words= 'english')\r\n",
        "bow = bow_vectorizer.fit_transform(dataset['Tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThAOfsl9k9u7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nApLmnLPrXaU"
      },
      "source": [
        "#Applying TF-IDF Vectorization to the Tweets\r\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words= 'english')\r\n",
        "tfidf = tfidf_vectorizer.fit_transform(dataset['Tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8NtyF1rXRV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdfFS1dorceF"
      },
      "source": [
        "https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZom9Q23rXBa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9_O6U0vrZgp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}